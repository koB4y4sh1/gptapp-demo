import asyncio
import logging
import os
import re
import random

from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, BrowserContext

from type.Image import ItemImage

# --- è¨­å®š ---
BASE_DOMAIN = "soco-st.com"
BASE_PAGE_URL = "https://soco-st.com/illust/page/"
MAX_DEPTH: int = 1 # ã‚¯ãƒ­ãƒ¼ãƒ«ã®æœ€å¤§æ·±åº¦
MAX_CONCURRENCY: int = 5 # ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§æ•°

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
visited = set()

def is_valid_path(url: str) -> bool:
    """
    URLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°ã€‚

    URLã®ã‚¢ãƒ³ã‚«ãƒ¼éƒ¨åˆ†ï¼ˆ#ä»¥é™ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: 'keyword'ï¼‰ã‚’å«ã‚€URLã¯ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚
    ã¾ãŸã€URLã®ãƒ›ã‚¹ãƒˆéƒ¨åˆ†ãŒBASE_DOMAINã«ä¸€è‡´ã—ã€ãƒ‘ã‚¹ãŒæ•´æ•°ã‚’å«ã‚€ãƒšãƒ¼ã‚¸URLã§ã‚ã‚Œã°æœ‰åŠ¹ã¨ã¿ãªã—ã¾ã™ã€‚
    ä¾‹: https://soco-st.com/24917 â†’ OK
        https://soco-st.com/24917#keyword_anchor â†’ NG ã‚¢ãƒ³ã‚«ãƒ¼ã¯å¯¾è±¡å¤–
        https://soco-st.com/category/tag â†’ NG

    Args:
        url (str): ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®URLã€‚

    Returns:
        bool: URLãŒæœ‰åŠ¹ãªå ´åˆã¯Trueã€ç„¡åŠ¹ãªå ´åˆã¯Falseã€‚
    """
    parsed = urlparse(url)
    return parsed.netloc.endswith(BASE_DOMAIN) and re.fullmatch(r"/\d+/?", parsed.path) and not parsed.fragment


async def crawl_page(html: str, url: str, context: BrowserContext, depth: int = 0, semaphore: asyncio.Semaphore = None) -> list[ItemImage]:
    """
    æŒ‡å®šã•ã‚ŒãŸHTMLã¨URLã‚’ã‚‚ã¨ã«ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ç”»åƒãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    - ãƒšãƒ¼ã‚¸å†…ã® 'color.png' ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’ç”»åƒã¨ã—ã¦æŠ½å‡ºã€‚
    - ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆh1.post_titleï¼‰ã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼ˆdiv.post_captionï¼‰ã‚‚å–å¾—ã€‚
    - URLãŒæœªè¨ªå•ã‹ã¤æœ‰åŠ¹ãªå ´åˆã®ã¿å†å¸°çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã€‚
    - depth ã«ã‚ˆã‚Šå†å¸°ã®æœ€å¤§æ·±åº¦ã‚’åˆ¶å¾¡ã€‚
    - `visited` ã‚»ãƒƒãƒˆã§URLã®é‡è¤‡ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é˜²æ­¢ï¼ˆä¸¦åˆ—å‡¦ç†ã®ãŸã‚å°†æ¥çš„ã«ã¯async-safeå¯¾å¿œãŒå¿…è¦ï¼‰ã€‚
    - `semaphore` ã«ã‚ˆã‚ŠåŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’åˆ¶é™ã€‚
    - Playwrightã® `BrowserContext` ã‚’ä½¿ã£ã¦å„ãƒªãƒ³ã‚¯ã‚’æ–°ã—ã„ãƒšãƒ¼ã‚¸ã§é–‹ãã€HTMLã‚’å–å¾—ã€‚

    Args:
        html (str): å¯¾è±¡ãƒšãƒ¼ã‚¸ã®HTMLæ–‡å­—åˆ—ã€‚
        url (str): å¯¾è±¡ãƒšãƒ¼ã‚¸ã®URLã€‚
        context (BrowserContext): Playwrightã«ã‚ˆã‚‹ãƒ–ãƒ©ã‚¦ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚
        depth (int): å†å¸°ã‚¯ãƒ­ãƒ¼ãƒ«ã®æ·±ã•ã€‚MAX_DEPTHä»¥ä¸Šã®å ´åˆã¯å†å¸°ã—ãªã„ã€‚
        semaphore (asyncio.Semaphore): ä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ç”¨ã®ã‚»ãƒãƒ•ã‚©ã€‚

    Returns:
        list[dict]: ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆï¼š
            - title: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
            - caption: ãƒšãƒ¼ã‚¸ã®èª¬æ˜æ–‡
            - images: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆcolor.pngã®ã¿ï¼‰
            - png_url: å¯¾å¿œã™ã‚‹ç”»åƒURLã®ãƒªã‚¹ãƒˆ
    """
    items = []

    async with semaphore:
        try:
            logging.debug(f"ğŸ“Œ {url} (depth={depth})")
            soup = BeautifulSoup(html, "html.parser")

            # ãƒ¡ã‚¿æƒ…å ±æŠ½å‡º
            title_tag = soup.find("h1", class_="post_title")
            caption_tag = soup.find("div", class_="post_caption")
            image_tags = soup.find_all("a", href=True)

            title = title_tag.get_text(strip=True) if title_tag else None
            caption = caption_tag.get_text(strip=True) if caption_tag else None
            png_url = [
                urljoin(url, a["href"]) for a in image_tags if a["href"].endswith("color.png")
            ]
            image_names = [os.path.basename(urlparse(u).path) for u in png_url]

            if title and caption and image_names:
                items.append(ItemImage(
                    title=title,
                    caption=caption,
                    images=png_url,
                    embedding=[]  # å¾Œå·¥ç¨‹ã§åŸ‹ã‚è¾¼ã¿ã‚’ä»˜ä¸
                ))
                logging.debug(f"ğŸ” PNGãƒªãƒ³ã‚¯ç™ºè¦‹: {png_url}")

            if depth >= MAX_DEPTH:
                logging.debug(f"â© æœ€å¤§æ·±åº¦ {MAX_DEPTH} ã«é”ã—ãŸãŸã‚ã€å†å¸°ã—ã¾ã›ã‚“: {url}")
                return items

            # å†å¸°ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URLã®åé›†ã¨ visited ãƒ•ã‚£ãƒ«ã‚¿
            ## æœ€å¤§æ·±åº¦ã‚’2ä»¥ä¸Šã«ã™ã‚‹å ´åˆã¯visitedã¸ã®è¿½åŠ ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            full_urls = [urljoin(url, a["href"]) for a in soup.find_all("a", href=True)]
            valid_full_urls = []
            for url in full_urls:
                if is_valid_path(url) and url not in visited:
                    visited.add(url)
                    valid_full_urls.append(url)

            # ä¸¦åˆ—å®Ÿè¡Œã®ã‚¿ã‚¹ã‚¯ä½œæˆ
            async def crawl_nested_page(target_url: str) -> list[ItemImage]:
                page = await context.new_page()
                try:
                    await page.goto(target_url, timeout=15000)
                    nested_html = await page.content()
                    nested_items = await crawl_page(
                        nested_html, target_url, context, depth + 1, semaphore
                    )
                    await asyncio.sleep(random.uniform(5.0, 10.0))
                    return nested_items
                except Exception as e:
                    logging.warning(f"âš ï¸ å†å¸°ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•— {target_url}: {e}")
                    return []
                finally:
                    await page.close()

            # asyncio.gather ã§ä¸¦åˆ—ã«ã‚¯ãƒ­ãƒ¼ãƒ« (æœ€å¤§ä¸¦åˆ—æ•°ã¯MAX_CONCURRENCY)
            tasks = [crawl_nested_page(u) for u in valid_full_urls]
            nested_results = await asyncio.gather(*tasks)

            for nested_items in nested_results:
                items.extend(nested_items)

        except Exception as e:
            logging.error(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•— {url}: {e}")

    return items


async def crawl_all_pages() -> list[ItemImage]:
    """
    ã‚µã‚¤ãƒˆå†…ã®ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€çµæœã‚’åé›†ã™ã‚‹é–¢æ•°ã€‚

    å„ãƒšãƒ¼ã‚¸ã‚’é †ç•ªã«å–å¾—ã—ã€ãã®ãƒšãƒ¼ã‚¸å†…ã§ç”»åƒã‚„ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    å†å¸°çš„ã«å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ã¾ã¨ã‚ã¦çµæœã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

    Returns:
        list[ItemPage]: ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®æƒ…å ±ï¼ˆcrawl_pageã®è¿”å´å€¤ã®ãƒªã‚¹ãƒˆï¼‰ã€‚
    """
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
    page_num = 1
    results: list[ItemImage] = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        while True:
            page_url = f"{BASE_PAGE_URL}{page_num}"
            logging.info(f"ğŸ“Œ ãƒšãƒ¼ã‚¸ç¢ºèªä¸­: {page_url}")

            try:
                await page.goto(page_url, timeout=15000)
                html = await page.content()

                if "ã‚¤ãƒ©ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚" in html:
                    logging.debug("âœ… ãƒšãƒ¼ã‚¸ã«ã‚¤ãƒ©ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                    break

                # 1ãƒšãƒ¼ã‚¸åˆ†ã®ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆå†å¸°ã‚‚å«ã‚ã‚‹ï¼‰
                items = await crawl_page(html, page_url, context, depth=0, semaphore=semaphore)
                results.extend(items)

                sleep_sec = random.uniform(1.0, 3.0)
                logging.debug(f"â¸ï¸ æ¬¡ã®ãƒšãƒ¼ã‚¸ã¾ã§ {sleep_sec:.2f} ç§’å¾…æ©Ÿä¸­...")
                await asyncio.sleep(sleep_sec)

                page_num += 1

            except Exception as e:
                logging.error(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— {page_url}: {e}")
                break

        await page.close()
        await context.close()
        await browser.close()
    return results


if __name__ == "__main__":
    asyncio.run(crawl_all_pages())
